\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc} 

% % Comment out for light mode
% \pagecolor[rgb]{0.1137, 0.1216, 0.1255}
% \color{white} 

\title{Deriving Backpropagation Gradients}
\author{Amelia}
\date{February 2026}

\begin{document}

\maketitle
\section{Deriving the gradients for a single value}
Let us derive the gradients that happen in backpropogation, one value at a time.

$$\text{let }\theta, W\in \mathbb R^n \text{ and } n \text{ be an arbitrary value}$$

\begin{figure}[!h]
\centering
\begin{tikzpicture}[
    shorten >=1pt,
    ->,
    draw=black!50,
    node distance=3cm and 1.5cm
]
\tikzstyle{unit}=[circle, draw, minimum size=18pt, inner sep=0pt]

% Input layer
\node[unit] (i1) {};
\node[unit, below=of i1] (in) {};
\node at ($(i1)!0.5!(in)$) {$\vdots$};
\node[above] at (i1.north) {$W_{i_1}+b_{i_1}$};
\node[below] at (in.south) {$W_{i_n}+b_{i_n}$};

% Hidden layer
\node[unit, right=of i1] (h1) {};
\node[unit, below=of h1] (hn) {};
\node at ($(h1)!0.5!(hn)$) {$\vdots$};
\node[above] at (h1.north) {$W_{h_1}+b_{h_1}$};
\node[below] at (hn.south) {$W_{h_n}+b_{h_n}$};

% Output layer
\node[unit, right=of h1, label=right:$o_1$] (o1) {};
\node[unit, below=of o1, label=right:$o_n$] (on) {};
\node at ($(o1)!0.5!(on)$) {$\vdots$};

% Connections
\draw (i1) -- (h1);
\draw (i1) -- (hn);
\draw (in) -- (h1);
\draw (in) -- (hn);

\draw (h1) -- (o1);
\draw (h1) -- (on);
\draw (hn) -- (o1);
\draw (hn) -- (on);

\end{tikzpicture}

    \caption{A typical Neural Net}
    \label{fig:1}
\end{figure}


\section{Adding the loss derivative}

Let $\mathcal L$ denote the loss. 
The update rule uses the gradient of the loss with respect to the variable we are tuning:


\begin{align}
W_{h_1} 
&\leftarrow W_{h_1} - \alpha \frac{\partial \mathcal L}{\partial W_{h_1}} 
% W_{h_1} 
% - \alpha \left( 
% \frac{\partial \mathcal L}{\partial o_1}
% \frac{\partial o_1}{\partial W_{h_1}}
% + \dots +
% \frac{\partial \mathcal L}{\partial o_n}
% \frac{\partial o_n}{\partial W_{h_1}}
% \right) \\
% &= 
\end{align}
To find $\frac{\partial \mathcal L}{\partial W_{h_1}}$, we can expand it with the chain rule
\begin{align}
\frac{\partial \mathcal L}{\partial W_{h_1}} &= \frac{\partial \mathcal L}{\partial o}\frac{\partial o}{\partial W_{h_1}}\\
&=
    \left(
\frac{\partial \mathcal L}{\partial o_1}
\frac{\partial o_1}{\partial W_{h_1}}
+ \dots +
\frac{\partial \mathcal L}{\partial o_n}
\frac{\partial o_n}{\partial W_{h_1}}
\right) 
\end{align}
Similarly for the bias:
\begin{align}
b_{h_1}
&\leftarrow b_{h_1}
- \alpha
\left(
\frac{\partial \mathcal L}{\partial o_1}
\frac{\partial o_1}{\partial b_{h_1}}
+ \dots +
\frac{\partial \mathcal L}{\partial o_n}
\frac{\partial o_n}{\partial b_{h_1}}
\right) \\
&= b_{h_1} - \alpha \frac{\partial \mathcal L}{\partial b_{h_1}}
\end{align}

\section{Deriving gradients in the vector case}
\begin{gather}\text{let }
    W_h \doteq 
    \begin{pmatrix}
        W_{h_1} \\
        \vdots\\
        W_{h_n}
    \end{pmatrix}
    \quad
    b_h \doteq 
    \begin{pmatrix}
        b_{h_1} \\
        \vdots\\
        b_{h_n}
    \end{pmatrix}
    \quad
    O_h \doteq 
    \begin{pmatrix}
        o_{h_1} \\
        \vdots\\
        o_{h_n}
    \end{pmatrix}
\end{gather}
By the update rule:
$$W_h \leftarrow W_{h} - \alpha \frac{\partial \mathcal L}{\partial W_h} $$


\begin{align}
\frac{\partial \mathcal L}{\partial W_h} &\doteq \begin{pmatrix}
\frac{\partial \mathcal L}{\partial W_{h_1}} \\
\vdots \\
\frac{\partial \mathcal L}{\partial W_{h_n}}
\end{pmatrix} \\
&= \begin{pmatrix}
\frac{\partial o_1}{\partial W_{h_1}} + \dots + \frac{\partial o_n}{\partial W_{h_1}} \\
\vdots \\
\frac{\partial o_1}{\partial W_{h_n}} + \dots + \frac{\partial o_n}{\partial W_{h_n}}
\end{pmatrix}
\end{align}

\section{Deriving gradients in earlier layers}

We are trying to find $\frac{\partial \mathcal L}{\partial W_i}$. Through the chain rule, we can expand this into:

$$\frac{\partial \mathcal{L}}{\partial W_i} =  \frac{\partial \mathcal L}{\partial o}\frac{\partial o}{\partial W_h}\frac{\partial W_h}{\partial W_i}$$


\end{document}
